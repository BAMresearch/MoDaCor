{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2f0df8",
   "metadata": {},
   "source": [
    "# Investigating the need for weighting / normalization vis-a-vis variance averaging\n",
    "\n",
    "## synopsis\n",
    "\n",
    "During the MoDaCor hackathon, the need for an additional normalisation matrix was argued by Jerome Kieffer. This document sets out to investigate and clarify that need, from both a mathematical as well as an empirical point of view. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "MoDaCor's core concept is that we can chain together modular operations in arbitrary sequence. The data ('BaseData' instance) is modified in place by each module, and generally does not change shape. Averaging operations are the exception to this rule, where the dimensionality of data can be changed during this operation. \n",
    "\n",
    "MoDaCor also propagates (a list of) uncertainty estimators in the form of variances (variances, i.e. the square of the uncertainties, are much faster to propagate). These variances can originate from multiple estimation sources. For example, the counting (Poisson) variance can be estimated at the start, but during an averaging operation an additional variance estimate can be gained from the square of the standard error on the mean (SEM). Further variance estimators may be added along the operations. Each should be propagated independently through the modules, until such a time that a combined estimator is desired from these individual estimators. \n",
    "\n",
    "All data must be accompanied with at least one variance estimator, as well as units ('dimensionless' if it does not have a unit). Operations with scalars (with uncertainties and units) are applied to the BaseData scalar rather than the signal matrix. \n",
    "\n",
    "The question now is whether BaseData's 'signal' matrix should be accompanied by an identically-sized 'normalization' or 'weighting' matrix, and how to deal with it in operations. The argument stems from the propagation of uncertainties in averaging operations, where sample size should likely be taken into consideration. This sample size is different per datapoint, and can be set, depending on factors such as detector pixel dimensions, number of pixels in an averaging bin, etc. \n",
    "\n",
    "For this document, we will not consider 'pixel splitting' operations, and consider each datapoint to be an indivisible unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87fd41",
   "metadata": {},
   "source": [
    "## investigating the effect of a weighting matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbce65",
   "metadata": {},
   "source": [
    "### The argument from different pixel dimensions\n",
    "\n",
    "Imagine we have a detector with two pixels, one of which has ten times the area of the other. pixel 1 has area $A_1 = 1$ with a variance $\\sigma^2_{A,1} = 0.01$, pixel 2 an area of $A_2 = 10$ with a variance $\\sigma^2_{A,2} = 0.04$. We are counting photons, so counting statistics apply. \n",
    "\n",
    "The first pixel collects $I_1 = 9$ counts, the second collects $I_2 = 100$ counts in the same period of time. The counting statistics-based uncertainties are therefore $\\sigma_1 = \\sqrt(I_1) = 3$ and $\\sigma_2 = \\sqrt(I_2) = 10$ counts, their variances $\\sigma^2_1 = u_1^2 = 9$ and $\\sigma^2_2 = u_2^2 = 100$, respectively. \n",
    "\n",
    "We do two steps to this data: one by area, and an averaging step, averaging over the two pixels. \n",
    "\n",
    "The normalization by pixel area $A$ has the following effect on the signal $I$ and the variance $\\sigma^2$:\n",
    "$$ \n",
    "I_{i, \\mathrm{out}} = I_{i, \\mathrm{in}} / A_i \\quad\\forall \\in \\set{1, ..., n}\n",
    "$$\n",
    "and using Goodman's expression [4] for the multiplication operations for the uncorrelated case: \n",
    "$$\n",
    "\\sigma^2_{i, \\mathrm{out}} = \\sigma^2_{i, \\mathrm{in}} A_i^2 + \\sigma^2_{A,i} I_{i, \\mathrm{in}}^2 + \\sigma^2_{i, \\mathrm{in}} \\sigma^2_{A,i} \\quad\\forall \\in \\set{1, ..., n}\n",
    "$$\n",
    "which, when also tracking optional weighting (by relative area in this case), would lead to: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "abc1148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical example, setup\n",
    "import numpy as np\n",
    "\n",
    "w  = np.array([1, 1], dtype=float) # weights\n",
    "w_unweighted  = np.array([1, 1], dtype=float) # weights matrix to be used for unweighted averages\n",
    "I  = np.array([9, 100], dtype=float) # signal\n",
    "v  = np.array([9, 100], dtype=float) # variance of signal\n",
    "A  = np.array([1, 10], dtype=float) # area to normalize by\n",
    "vA = np.array([0.01, 0.04], dtype=float) # variance of area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f859ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 1\n",
    "I_out = I / A\n",
    "f = 1/A # multiplier for Goodman's algo\n",
    "vf = f**2 * (np.sqrt(vA)/A)**2 # variance of the multiplier for Goodman's algo\n",
    "w *= A\n",
    "v_goodman = v * f**2 + vf * I**2 + v*vf\n",
    "v = I_out**2 * ((np.sqrt(vA) / A)**2 + (np.sqrt(v)/ I)**2) # ignoring cross-term\n",
    "I=I_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb1e4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel: 0: signal I: 9.0, variance v: 9.81, goodman variance v: 9.9, weight w: 1.0\n",
      "pixel: 1: signal I: 10.0, variance v: 1.04, goodman variance v: 1.0404000000000002, weight w: 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f'pixel: {i}: signal I: {I[i]}, variance v: {v[i]}, goodman variance v: {v_goodman[i]}, weight w: {w[i]}') for i in range(len(I))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54172169",
   "metadata": {},
   "source": [
    "\n",
    "|       | signal $I$ | variance $v$ | weighting $w_i$ |\n",
    "| ----- | ---------- | ------------ | --------------- |\n",
    "|pixel 1| 9          | 9.9          | 1               |\n",
    "|pixel 2| 10         | 1.04         | 10              |\n",
    "\n",
    "The average signal $\\hat{I}$ would then follow from equation 12 in [3]: \n",
    "\n",
    "$$ \n",
    "\\hat{I} = \\frac{1}{\\sum_i w_i} \\sum_i w_i I_i\n",
    "$$\n",
    "a new variance estimator $\\sigma^2_{\\mathrm{av}}$ is obtained from equation 13 in [3] (note: not sure why the original paper calls this a \"(Co)variance\", probably because of the mixin of weights?): \n",
    "\n",
    "(note, this seems weirdly lacking in squared weights.)\n",
    "$$\n",
    "\\sigma^2_{\\mathrm{av}} = \\frac{1}{\\sum_i w_i} \\sum_i w_i (I_i - \\hat{I}) (w_i - \\hat{w})\n",
    "$$\n",
    "\n",
    "and the propagated variance should be:\n",
    "\n",
    "$$ \n",
    "\\sigma^2 = \\frac{1}{\\sum_i w_i^2} \\sum_i w_i^2 \\sigma^2_{i, \\mathrm{in}}\n",
    "$$\n",
    "\n",
    "\n",
    "for the simple average, $w_i = 1$, for the weighted average, we would assume the weighting in the previous table. \n",
    "\n",
    "The numerical example then becomes: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "89e8ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_av = w\n",
    "\n",
    "I_hat = 1/w_av.sum() * (w_av * I).sum()\n",
    "v_hat = 1/(w_av**2).sum() * (w_av**2 * v).sum()\n",
    "v_est = 1/w_av.sum() * (w_av * np.abs(I-I_hat) * np.abs(w_av - w_av.mean())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d8d40d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weighted average: I_hat: 9.90909090909091, variance v_hat: 1.126831683168317, estimated variance v_est: 0.7438016528925584\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nweighted average: I_hat: {I_hat}, variance v_hat: {v_hat}, estimated variance v_est: {v_est}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0855168",
   "metadata": {},
   "source": [
    "Alternatively, we can use inverse-variance weighting on the datapoints to obtain the mean with the smallest variance, according to [6]: \n",
    "\n",
    "$$\n",
    "\\hat{I} = \\frac{\\sum_i I_i / \\sigma_i^2}{\\sum_i 1/\\sigma_i^2} \n",
    "$$\n",
    "\n",
    "which has a variance of\n",
    "$$ \n",
    "\\sigma^2 = \\frac{1}{\\sum_i 1/ \\sigma_i^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f091c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse variance weighted average: I_hat: 9.90, inverse variance weighted variance: 0.94\n"
     ]
    }
   ],
   "source": [
    "v_av = v_goodman\n",
    "I_hat = (I / v_av).sum() / (1/v_av).sum()\n",
    "v_hat = 1 / (1/v_av).sum()\n",
    "print(f'inverse variance weighted average: I_hat: {I_hat:0.02f}, inverse variance weighted variance: {v_hat:0.02f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d1d45",
   "metadata": {},
   "source": [
    "\n",
    "|         | signal $I$ | variance $v$ | new var. est $v_new$ |\n",
    "| -----   | ---------- | ------------ | --------------- |\n",
    "| unweighted average | 9.5        |  5.425       |    0.0          |\n",
    "|   weighted average |  9.91       |   1.127      |    0.744         |\n",
    "| inv. var. wt. av.  | 9.90        | 0.94 | - |\n",
    "\n",
    "#### Literature sources: \n",
    "  1. https://veritas.ucd.ie/~apl/labs_master/docs/2020/DA/Matrix-Methods-for-Error-Propagation.pdf\n",
    "  2. https://en.wikipedia.org/wiki/Propagation_of_uncertainty\n",
    "  3. https://ds.ifi.uni-heidelberg.de/files/Team/eschubert/publications/SSDBM18-covariance-authorcopy.pdf\n",
    "  4. https://www.tandfonline.com/doi/abs/10.1080/01621459.1960.10483369\n",
    "  5. https://en.wikipedia.org/wiki/Coefficient_of_variation\n",
    "  6. https://en.m.wikipedia.org/wiki/Inverse-variance_weighting\n",
    "  7. https://journals.iucr.org/j/issues/2025/01/00/jo5109/index.html\n",
    "\n",
    "Apropos, this one is wild, and might be worth a decent look: \n",
    "@MISC {454266,\n",
    "    TITLE = {How can I calculate uncertainty of the mean of a set of samples with different uncertainties?},\n",
    "    AUTHOR = {whuber (https://stats.stackexchange.com/users/919/whuber)},\n",
    "    HOWPUBLISHED = {Cross Validated},\n",
    "    NOTE = {URL:https://stats.stackexchange.com/q/454266 (version: 2022-08-02)},\n",
    "    EPRINT = {https://stats.stackexchange.com/q/454266},\n",
    "    URL = {https://stats.stackexchange.com/q/454266}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7558d6",
   "metadata": {},
   "source": [
    "## Comparison with Jerome's equations\n",
    "\n",
    "Let's check if we are compatible with the method implemented in PyFAI for the weight-averaged intensity (eq. 2 in [7]): \n",
    "$$\n",
    "\\hat{I_J} = \\frac{\\sum_i I_i}{\\sum_i \\mathrm{norm_i}} \n",
    "$$\n",
    "where $\\mathrm{norm_i}$ contains the normalizations (pixel area in our example), and: \n",
    "$$ \n",
    "\\hat{\\sigma_J^2} = \\frac{\\sum_i \\sigma^2_i}{\\sum_i \\mathrm{norm_i}} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "216d2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerome's method: I_hat_J: 9.91, v_hat_J: 9.91\n"
     ]
    }
   ],
   "source": [
    "I  = np.array([9, 100], dtype=float) # signal\n",
    "v  = np.array([9, 100], dtype=float) # variance of signal\n",
    "w  = np.array([1, 10], dtype=float) # weights\n",
    "I_hat_J = I.sum() / w.sum()\n",
    "v_hat_J = v.sum() / w.sum()\n",
    "print(f\"Jerome's method: I_hat_J: {I_hat_J:0.02f}, v_hat_J: {v_hat_J:0.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4fb73",
   "metadata": {},
   "source": [
    "# on BaseData\n",
    "BaseData has become somewhat unwieldy for calculations / modules, with three sets of units, and a nonintuitive use of normalization. \n",
    "\n",
    "I therefore propose the following structure: \n",
    "\n",
    "BaseData:\n",
    " - Data elements:\n",
    "   - signal: np.ndarray\n",
    "   - variances: dict (should we define some default names?)\n",
    "   - weights: 1 (default) or np.ndarray when set, only used for weighted averaging operations\n",
    "   - mask: 0 (default) or np.ndarray, dtype uint8 of size signal, only used for skipping datapoints when not 0\n",
    "   - scalar: float, default 1\n",
    "   - scalar_variance: float, default 0\n",
    " - Metadata elements:\n",
    "   - units: pint.Unit, for signal*scalar\n",
    "   - rank_of_data: int, default: min(2, signal.ndim)\n",
    "   - axes: list[Self|None], points at other BaseData elements in the DataBundle (when available, what to do with things like \"temperature\" or \"stage position\"?). \n",
    "\n",
    "For single values read in from metadata elements in the files, the following must be set:\n",
    "BaseData (single element):\n",
    " - Data elements:\n",
    "   - signal: float (will act on scalar for multiplication/division operations, otherwise on signal)\n",
    "   - variances: {'var': float}\n",
    " - Metadata elements:\n",
    "   - units: pint.Unit\n",
    "   - rank_of_data: 0 (set automatically when signal is not np.ndarray)\n",
    "\n",
    "Implementation notes:\n",
    "* operations should act between BaseData and BaseData objects\n",
    "* weights will be used for weighted averaging, and can be set to 1/normalization for Jerome's algorithm. weighted averages are normalized to 1/(sum(weights)) to avoid impacting the scaling. \n",
    "* an alternative averaging step can be weighted by the inverse variance of the datapoint. See reference [6]\n",
    "* a separate step will apply scalar to the signal, after which scalar and scalar_variance are normalized to scalar. \n",
    "* there is only one units to take care of, that's the units of the signal*scalar\n",
    "* the propagation of variances in a weighted averaging step is apparently not unambiguously defined. this might need some thought. \n",
    "* we will have a library of basic maths operations (multiplication, division, addition and subtraction, as well as some trigionometric methods for calculating q, psi) which will have variance propagation implemented. \n",
    "* whether it is more beneficial to propagate variance or uncertainty depends on the mathematical operation. variance is, however, more unambiguously defined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d5532e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., 30., 48.],\n",
       "       [ 2.,  6., 12.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[1, 10, 12], [1,2,3]], dtype=float) # area to normalize by\n",
    "q = np.array(4, dtype=float) # weights for area\n",
    "\n",
    "p = np.array([2, 3, 4], dtype=float) # signal\n",
    "Q*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a3e816cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_shapes(Q.shape, p.shape)  # This will raise an error if the shapes are not compatible for broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ab89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
